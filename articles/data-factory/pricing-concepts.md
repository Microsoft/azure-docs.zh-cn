---
title: 通过示例了解 Azure 数据工厂定价
description: 本文使用详细的示例介绍并演示 Azure 数据工厂定价模型
documentationcenter: ''
author: dcstwh
ms.author: weetok
manager: jroth
ms.reviewer: maghan
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 09/14/2020
ms.openlocfilehash: cea8ae07585f09c644f0ef6e1e6142998ddc7f08
ms.sourcegitcommit: d60976768dec91724d94430fb6fc9498fdc1db37
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 12/02/2020
ms.locfileid: "96497494"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>通过示例了解数据工厂定价

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

本文使用详细的示例介绍并演示 Azure 数据工厂定价模型。

> [!NOTE]
> 以下这些示例中使用的价格是假设的，并不意味着实际定价。

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储

在此方案中，需按计划将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储。

若要完成此方案，需使用以下项创建一个管道：

1. 使用输入数据集（适用于将要从 AWS S3 复制的数据）的复制活动。

2. Azure 存储上的数据的输出数据集。

3. 一个计划触发器，用于每隔一小时执行一次管道。

   ![此图显示了具有计划触发器的管道。 在此管道中，复制活动可通过流向输入数据集来流向 AWS S3 链接服务，还可以通过流向输出数据集来流向 Azure 存储链接服务。](media/pricing-concepts/scenario1.png)

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 2 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 2 个活动运行（1 个用于触发器运行，1 个用于活动运行） |
| 复制数据假设：执行时间 = 10 分钟 | 10 \* 4 Azure Integration Runtime（默认 DIU 设置 = 4）有关数据集成单元和副本性能优化的详细信息，请参阅[此文](copy-activity-performance.md) |
| 监视管道假设：仅发生 1 次运行 | 检索到 2 个监视运行记录（1 个用于管道运行，1 个用于活动运行） |

**方案定价总计：$0.16811**

- 数据工厂操作 = **$0.0001**
  - 读取/写入 = 10\*00001 = $0.0001 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 2\*000005 = $0.00001 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$0.168**
  - 活动运行 = 001\*2 = 0.002 [1 运行 = $1/1000 = 0.001]
  - 数据移动活动 = $0.166（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.25/小时）

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>使用 Azure Databricks 按小时复制数据并进行转换

在此方案中，需使用 Azure Databricks 按计划将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储并对数据进行转换。

若要完成此方案，需使用以下项创建一个管道：

1. 一个使用输入数据集（适用于将要从 AWS S3 复制的数据）和输出数据集（适用于 Azure 存储上的数据）的复制活动。
2. 一个用于数据转换的 Azure Databricks 活动。
3. 一个计划触发器，用于每隔一小时执行一次管道。

![此图显示了具有计划触发器的管道。 在此管道中，复制活动会流向输入数据集、输出数据集和 DataBricks 活动（在 Azure Databricks 上运行）。 输入数据集会流向 AWS S3 链接服务。 输出数据集会流向 Azure 存储链接服务。](media/pricing-concepts/scenario2.png)

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 3 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 3 个活动运行（1 个用于触发器运行，2 个用于活动运行） |
| 复制数据假设：执行时间 = 10 分钟 | 10 \* 4 Azure Integration Runtime（默认 DIU 设置 = 4）有关数据集成单元和副本性能优化的详细信息，请参阅[此文](copy-activity-performance.md) |
| 监视管道假设：仅发生 1 次运行 | 检索到 3 个监视运行记录（1 个用于管道运行，2 个用于活动运行） |
| 执行 Databricks 活动假设：执行时间 = 10 分钟 | 10 分钟执行外部管道活动 |

**方案定价总计：$0.16916**

- 数据工厂操作 = **$0.00012**
  - 读取/写入 = 11\*00001 = $0.00011 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 3\*000005 = $0.00001 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$0.16904**
  - 活动运行 = 001\*3 = 0.003 [1 运行 = $1/1000 = 0.001]
  - 数据移动活动 = $0.166（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.25/小时）
  - 外部管道活动 = $0.000041（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.00025/小时）

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>使用动态参数按小时复制数据并进行转换

在此方案中，需使用 Azure Databricks（使用脚本中的动态参数）按计划将数据每隔一小时从 AWS S3 复制到 Azure Blob 存储并进行转换。

若要完成此方案，需使用以下项创建一个管道：

1. 一个使用输入数据集（适用于将要从 AWS S3 复制的数据）和输出数据集（适用于 Azure 存储上的数据）的复制活动。
2. 一个查找活动，用于将参数动态传递到转换脚本。
3. 一个用于数据转换的 Azure Databricks 活动。
4. 一个计划触发器，用于每隔一小时执行一次管道。

![此图显示了具有计划触发器的管道。 在此管道中，复制活动会流向输入数据集、输出数据集和查找活动（流向在 Azure Databricks 上运行的 DataBricks 活动）。 输入数据集会流向 AWS S3 链接服务。 输出数据集会流向 Azure 存储链接服务。](media/pricing-concepts/scenario3.png)

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 3 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 4 个活动运行（1 个用于触发器运行，3 个用于活动运行） |
| 复制数据假设：执行时间 = 10 分钟 | 10 \* 4 Azure Integration Runtime（默认 DIU 设置 = 4）有关数据集成单元和副本性能优化的详细信息，请参阅[此文](copy-activity-performance.md) |
| 监视管道假设：仅发生 1 次运行 | 检索到 4 个监视运行记录（1 个用于管道运行，3 个用于活动运行） |
| 执行查找活动假设：执行时间 = 1 分钟 | 1 分钟执行管道活动 |
| 执行 Databricks 活动假设：执行时间 = 10 分钟 | 10 分钟执行外部管道活动 |

**方案定价总计：$0.17020**

- 数据工厂操作 = **$0.00013**
  - 读取/写入 = 11\*00001 = $0.00011 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 4\*000005 = $0.00002 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$0.17007**
  - 活动运行 = 001\*4 = 0.004 [1 运行 = $1/1000 = 0.001]
  - 数据移动活动 = $0.166（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.25/小时）
  - 管道活动 = $0.00003（以 1 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.002/小时）
  - 外部管道活动 = $0.000041（以 10 分钟的执行时间按比例计算。 Azure Integration Runtime 上的定价为 $0.00025/小时）

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>对普通 workday 使用映射数据流调试

作为数据工程人员，Sam 负责设计、构建和测试每天映射数据流。 Sam 在早上登录到 ADF UI，并为数据流启用调试模式。 调试会话的默认 TTL 为60分钟。 Sam 在一整天内工作8小时，因此调试会话永不过期。 因此，Sam 的费用将为：

**8 (小时) x 8 (计算优化内核) x $0.193 = $12.35**

同时，Chris 另外，Chris 还会登录到 ADF 浏览器 UI，进行数据事件探查和 ETL 设计工作。 丽丽在 ADF 中的所有日子都不起作用。 丽丽只需在同一时间段内使用数据流调试程序1小时，在同一天内使用 Sam。 这些费用是丽丽为调试使用而产生的费用：

**1 (小时) x 8 (常规用途核心) x $0.274 = $2.19**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>在 blob 存储区中转换数据和映射数据流

在这种情况下，你想要在 ADF 中直观地转换 Blob 存储中的数据，按小时计划。

若要完成此方案，需使用以下项创建一个管道：

1. 带有转换逻辑的数据流活动。

2. Azure 存储上数据的输入数据集。

3. Azure 存储上的数据的输出数据集。

4. 一个计划触发器，用于每隔一小时执行一次管道。

| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 2 个读/写实体  |
| 创建数据集 | 4 个读/写实体（2 个用于创建数据集，2 个用于链接的服务的引用） |
| 创建管道 | 3 个读/写实体（1 个用于创建管道，2 个用于数据集引用） |
| 获取管道 | 1 个读/写实体 |
| 运行管道 | 2 个活动运行（1 个用于触发器运行，1 个用于活动运行） |
| 数据流假设：执行时间 = 10 分钟 + 10 分钟 TTL | \*带有 TTL 10 的一般计算的 10 16 核心 |
| 监视管道假设：仅发生 1 次运行 | 检索到 2 个监视运行记录（1 个用于管道运行，1 个用于活动运行） |

**方案总定价： $1.4631**

- 数据工厂操作 = **$0.0001**
  - 读取/写入 = 10\*00001 = $0.0001 [1 读/写 = $0.50/50000 = 0.00001]
  - 监视  = 2\*000005 = $0.00001 [1 监视 = $0.25/50000 = 0.000005]
- 管道业务流程 &amp; 执行 = **$1.463**
  - 活动运行 = 001\*2 = 0.002 [1 运行 = $1/1000 = 0.001]
  - 数据流活动 = $1.461 按比例20分钟 (10 分钟执行时间 + 10 分钟 TTL) 。 Azure Integration Runtime 上的 $ 0.274/小时，具有16个核心一般计算

## <a name="data-integration-in-azure-data-factory-managed-vnet"></a>Azure 数据工厂托管 VNET 中的数据集成
在这种情况下，你想要删除 Azure Blob 存储中的原始文件并将数据从 Azure SQL 数据库复制到 Azure Blob 存储。 你将在不同的管道上执行两次此执行。 这两个管道的执行时间重叠。
![Scenario4 ](media/pricing-concepts/scenario-4.png) 若要实现此方案，需要创建具有以下项的两个管道：
  - 管道活动–删除活动。
  - 一个复制活动，其中包含要从 Azure Blob 存储复制的数据的输入数据集。
  - 用于 Azure SQL 数据库上的数据的输出数据集。
  - 用于执行管道的计划触发器。


| **操作** | **类型和单元** |
| --- | --- |
| 创建链接的服务 | 4读取/写入实体 |
| 创建数据集 | 8个读取/写入实体 (4 个，用于创建数据集，4个用于链接服务引用)  |
| 创建管道 | 6用于创建管道的 (2 读取/写入实体，4表示数据集引用)  |
| 获取管道 | 2 个读/写实体 |
| 运行管道 | 6活动运行 (2 用于触发器运行，4表示活动运行)  |
| 执行删除活动：每个执行时间 = 5 分钟第一条管道中的 Delete 活动执行时间为 10:00 AM UTC 到 10:05 AM UTC。 第二个管道中的 Delete 活动执行时间为 10:02 AM UTC 到 10:07 AM UTC。|托管 VNET 中7分钟内的最小活动执行。 在托管 VNET 中，管道活动最多支持50并发。 |
| 复制数据假设：每次执行时间 = 10 分钟。第一条管道中的复制执行时间为 10:06 AM UTC 到 10:15 AM UTC。 第二个管道中的 Delete 活动执行时间为 10:08 AM UTC 到 10:17 AM UTC。 | 10 * 4 Azure Integration Runtime (默认 DIU 设置 = 4) 有关数据集成单元和优化复制性能的详细信息，请参阅 [此文](copy-activity-performance.md) |
| 监视管道假设：只发生2个运行 | 6 (2 的监视运行记录检索到管道运行，4表示活动运行)  |


**方案总定价： $0.45523**

- 数据工厂操作 = $0.00023
  - 读/写 = 20 * 00001 = $0.0002 [1 R/W = $ 0.50/50000 = 0.00001]
  - 监视 = 6 * 000005 = $0.00003 [1 监视 = $ 0.25/50000 = 0.000005]
- 管道业务流程 & 执行 = $0.455
  - 活动运行 = 0.001 * 6 = 0.006 [1 运行 = $ 1/1000 = 0.001]
  - 数据移动活动 = $0.333 (按比例执行时间为10分钟。 Azure Integration Runtime 上的定价为 $0.25/小时）
  - 管道活动 = $0.116 (按7分钟执行时间。 Azure Integration Runtime 上的 $ 1/小时) 

> [!NOTE]
> 这些价格仅用于示例目的。

**常见问题解答**

问：如果我想要运行超过50的管道活动，可以同时执行这些活动吗？

答：允许最大50并发管道活动。  51th 管道活动将排入队列，直到打开 "自由槽"。 对于外部活动是相同的。 允许最大800个并发外部活动。

## <a name="next-steps"></a>后续步骤

了解 Azure 数据工厂的定价以后，即可开始操作！

- [使用 Azure 数据工厂 UI 创建数据工厂](quickstart-create-data-factory-portal.md)

- [Azure 数据工厂简介](introduction.md)

- [Azure 数据工厂中的视觉对象创作](author-visually.md)
